<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Swarm on Docker Pirates ARMed with explosive stuff</title>
    <link>http://localhost:1313/categories/swarm/</link>
    <description>Recent content in Swarm on Docker Pirates ARMed with explosive stuff</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Mar 2016 18:40:04 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/swarm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Let&#39;s build a PicoCluster for Docker Swarm</title>
      <link>http://localhost:1313/post/lets-build-a-pi-docker-picocluster/</link>
      <pubDate>Wed, 23 Mar 2016 18:40:04 +0100</pubDate>
      
      <guid>http://localhost:1313/post/lets-build-a-pi-docker-picocluster/</guid>
      <description>

&lt;p&gt;As we love to use Docker Swarm on a cluster of Raspberry Pi&amp;rsquo;s, we&amp;rsquo;d like to cover
in this hands-on tutorial how to build such a cluster easily with a hardware kit
from &lt;a href=&#34;http://picocluster.com&#34;&gt;PicoCluster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All you need is a PicoCluster kit for a 3-node or 5-node cluster, a couple of
Raspberry Pi&amp;rsquo;s and the time to assemble all the parts together. The project can be
completed within an hour only, and makes so much fun, too - especially when you can
share this as quality time with your kids.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-pdu-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PicoCluster had just sent us two different kits of their cool Raspberry Pi
clusters. Of course, we are eager to build these new clusters so you can get a first impression.&lt;/p&gt;

&lt;h3 id=&#34;what-s-in-the-box:098640bbbda183ea709087a458ce9550&#34;&gt;What&amp;rsquo;s in the box&lt;/h3&gt;

&lt;p&gt;3-node PicoCluster kit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acrylic case, all parts&lt;/li&gt;
&lt;li&gt;PDU (12V input, 4x USB 5V output), including all wires&lt;/li&gt;
&lt;li&gt;wiring for 3x microUSB power&lt;/li&gt;
&lt;li&gt;case wiring for 1x HDMI&lt;/li&gt;
&lt;li&gt;case wiring for 1x 12V power plug&lt;/li&gt;
&lt;li&gt;base mounting for Pi tower, including all crews, nuts &amp;amp; bolts, spacers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-parts.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5-node PicoCluster kit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acrylic case, all parts&lt;/li&gt;
&lt;li&gt;internal 8-port Ethernet switch&lt;/li&gt;
&lt;li&gt;PDU (12V input, 5x USB 5V output), including all wires&lt;/li&gt;
&lt;li&gt;wiring for 5x microUSB power&lt;/li&gt;
&lt;li&gt;wiring for network (Raspberry Pi to Ethernet switch)&lt;/li&gt;
&lt;li&gt;case wiring for 1x Ethernet&lt;/li&gt;
&lt;li&gt;case wiring for 1x HDMI&lt;/li&gt;
&lt;li&gt;case wiring for 2x USB&lt;/li&gt;
&lt;li&gt;case wiring for 1x 12V power plug&lt;/li&gt;
&lt;li&gt;base mounting for Pi tower, including all crews, nuts &amp;amp; bolts, spacers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-parts.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not included in the kits: (so you have to buy it separately)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi‚Äôs and microSD cards&lt;/li&gt;
&lt;li&gt;AC adapter (12V, 1.5A, barrel plug 5.5x2.1mm)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally we&amp;rsquo;ll need some common tools for the mechanical assembling:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;philips screw driver&lt;/li&gt;
&lt;li&gt;7mm wrench (or a pliers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/tools-screwdriver-wrench.jpg&#34; alt=&#34;PicoCluster toosl&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;assembling-the-3-node-picocluster:098640bbbda183ea709087a458ce9550&#34;&gt;Assembling the 3-node PicoCluster&lt;/h3&gt;

&lt;h4 id=&#34;towering-the-raspberry-pi-s:098640bbbda183ea709087a458ce9550&#34;&gt;Towering the Raspberry Pi&amp;rsquo;s&lt;/h4&gt;

&lt;p&gt;First we take the mounting plate for the Pi tower and stack all
three Raspberry Pi&amp;rsquo;s with the help of the included spacers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-pi-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;wiring-all-together:098640bbbda183ea709087a458ce9550&#34;&gt;Wiring all together&lt;/h4&gt;

&lt;p&gt;Now it&amp;rsquo;s easy to mount the PDU on top of the Pi tower with 4 screws and attaching
the USB power cables to the three Raspberry Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-pdu-tower2.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Next we mount the tower on the base plate, which makes our new cluster look even better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-base-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From now on, it&amp;rsquo;s easy to complete the cluster installation: We connect
the 12V power cable to the PDU and front cover. We can also attach the HDMI cable
to the upper Raspberry Pi and mount the other end to the front cover, too.&lt;/p&gt;

&lt;h4 id=&#34;assembling-the-case:098640bbbda183ea709087a458ce9550&#34;&gt;Assembling the case&lt;/h4&gt;

&lt;p&gt;Finally, we mount the side and back cover and put on the top plate.
Mounting with nuts &amp;amp; bolts and the PicoCluster is ready.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Don&amp;rsquo;t forget to flash your microSD cards and insert them into the Pi&amp;rsquo;s,
because as soon as we close the cluster case, it&amp;rsquo;s a little bit harder to change
the SD cards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-completed.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;assembling-the-5-node-picocluster:098640bbbda183ea709087a458ce9550&#34;&gt;Assembling the 5-node PicoCluster&lt;/h3&gt;

&lt;h4 id=&#34;towering-the-raspberry-pi-s-1:098640bbbda183ea709087a458ce9550&#34;&gt;Towering the Raspberry Pi&amp;rsquo;s&lt;/h4&gt;

&lt;p&gt;With the 5-node cluster we start similar as with the 3-node. We take
the mounting plate for the Pi tower and stack all five Raspberry Pi&amp;rsquo;s with the
spacers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-pi-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-pi-tower2.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;wiring-all-together-1:098640bbbda183ea709087a458ce9550&#34;&gt;Wiring all together&lt;/h4&gt;

&lt;p&gt;Next, we mount the PDU on top of the Pi tower with 4 screws and attach
the USB power cables to the five Raspberry Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-pdu-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As soon as we mount the tower on the base plate, we see that we do have a lot more
parts left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-base-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the 5-node cluster we have an internal dedicated 8-port switch, which we&amp;rsquo;ll
install inside the case and wire all the ethernet cables to the Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-ethernet-switch.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From now on, it&amp;rsquo;s easy to complete the cluster installation: We connect
the 12V power cable to the PDU and front cover. We can also attach the HDMI cable
to the upper Raspberry Pi and mount the other end to the front cover, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-5node-switch-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll also find two USB cables in the kit, which you can connect to one or two
different Pi&amp;rsquo;s and mount the plug to the front panel as well. Attaching HDMI and USB
plugs to the case is really optional.&lt;/p&gt;

&lt;h4 id=&#34;assembling-the-case-1:098640bbbda183ea709087a458ce9550&#34;&gt;Assembling the case&lt;/h4&gt;

&lt;p&gt;Finally, we mount the side and back cover and put on the top plate.
Mounting with nuts &amp;amp; bolts and our second PicoCluster is almost ready. As there are a
few more parts to assemble the 5-node cluster, it will take a little bit longer than
building the smaller cluster.&lt;/p&gt;

&lt;h3 id=&#34;finally-we-have-two-new-clusters:098640bbbda183ea709087a458ce9550&#34;&gt;Finally, we have two new clusters&lt;/h3&gt;

&lt;p&gt;At the end we have now built two new Raspberry Pi clusters, a 3-node and a 5-node
from PicoCluster. Both are looking really neat and everything is stowed away in
a perfect way. Only accessing the microSD cards is not optimal once after the case
is closed. You just have to remove only a few screws and dismounting the rear side
panel of the case and then you can easily access the SD card slots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/picocluster-kits/picocluster-3node-and-5node-cluster.jpg&#34; alt=&#34;PicoCluster 3- and 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Because the 5-node cluster includes an integrated 8-port Ethernet switch, we&amp;rsquo;d recommend
it to use as a standalone Pi cluster for experimenting with Docker Swarm and other
cluster related tutorials and demos. You just have to attach a 12V power source and
a single network link. This is all you need to start playing with it right away.&lt;/p&gt;

&lt;p&gt;Now, you can go ahead and install software on your new PicoCluster.
However, this isn&amp;rsquo;t within the scope of this hands-on project, so we&amp;rsquo;ll point you to one of our
latest tutorials where you learn &lt;a href=&#34;http://blog.hypriot.com/post/how-to-setup-rpi-docker-swarm/&#34;&gt;how to setup a Docker Swarm cluster with Raspberry Pi&amp;rsquo;s&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;where-can-you-get-it:098640bbbda183ea709087a458ce9550&#34;&gt;Where can you get it ?&lt;/h3&gt;

&lt;p&gt;You can order your own ready-to-use and tiny data center directly at &lt;a href=&#34;http://picocluster.com&#34;&gt;PicoCluster&lt;/a&gt;
with the Raspberry Pi&amp;rsquo;s included. The hardware kits we used in this post should be available soon, too.
PicoCluster is currently optimizing a few parts like the PDU to get more power for the new Raspberry Pi 3,
so we expect the next version will get some changes and improvements compared to these
beta units.&lt;/p&gt;

&lt;p&gt;Please send us your feedback on our &lt;a href=&#34;https://gitter.im/hypriot/talk&#34;&gt;Gitter channel&lt;/a&gt; or tweet your thoughts and ideas on this project at &lt;a href=&#34;https://twitter.com/HypriotTweets&#34;&gt;@HypriotTweets&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dieter &lt;a href=&#34;https://twitter.com/Quintus23M&#34;&gt;@Quintus23M&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up 100 nodes Jenkins cluster with Docker Swarm in less than 10 minutes</title>
      <link>http://localhost:1313/post/setting-up-100-nodes-jenkins-cluster-with-docker-swarm-in-less-than-10-minutes/</link>
      <pubDate>Sun, 20 Mar 2016 23:00:00 +0100</pubDate>
      
      <guid>http://localhost:1313/post/setting-up-100-nodes-jenkins-cluster-with-docker-swarm-in-less-than-10-minutes/</guid>
      <description>

&lt;p&gt;A week ago Scaleway announced their new C2 server. I was so excited to see that they were introducing new ARM C2 servers with 8 CPUs and 32 GB of RAM. Wow!
Not until I logged in and did &lt;code&gt;lscpu&lt;/code&gt; did I realize that those servers were &amp;ldquo;just&amp;rdquo; Intel Atom servers.&lt;/p&gt;

&lt;p&gt;A bit disappointed I thought what to do now? In my enthusiasm I had spun up 3 servers with 82 GB of RAM and 24 CPUs.
Still with my thoughts firmly rooted in the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-join-your-first-swarm/&#34;&gt;Docker #SwarmWeek&lt;/a&gt; I could not resist to find out how easy it would be to set up a Swarm Cluster on those servers.&lt;/p&gt;

&lt;p&gt;But a Swarm Cluster on its own is like a container ship without any containers - so I decided to run a Jenkins Cluster on top of the Swarm Cluster.&lt;/p&gt;

&lt;p&gt;You might not believe me, but I was able to set up a 100 node Jenkins Cluster in less than 10 minutes.
And you can do that, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/100-nodes-jenkins/scaleway.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are couple of reasons why it was so simple.&lt;/p&gt;

&lt;h2 id=&#34;meet-the-new-scaleway-c2-servers:42ed33cefda8a6f74e4e47b0527e1f17&#34;&gt;Meet the new Scaleway C2 servers&lt;/h2&gt;

&lt;p&gt;The first reason is that I was using the new Scaleway Cloud servers.&lt;/p&gt;

&lt;p&gt;You might think &amp;ldquo;well, just ordinary cloudservers like those Amazon EC2 instances&amp;rdquo;, but the Scaleway offering is really quite different.
By buying a Scaleway server you get access to real hardware that only belongs to you. It is not a virtual machine running on shared hardware.
Still you have all the flexibilty of an - say Amazon EC2 instance - you can start and stop the instance on demand and pay as you go on an hourly rate.
Furthermore you can completely control the Scaleway servers with an API and a great command line interface.&lt;/p&gt;

&lt;p&gt;Did I already talk about the pricing model?&lt;/p&gt;

&lt;p&gt;As the Scaleway offering feels quite similar to Amazons AWS EC2 offering I looked for an instance type that was more or less comparable to the top model from Scaleway.
I ended up with an &lt;a href=&#34;http://www.ec2instances.info/?selected=m4.2xlarge&#34;&gt;m4.2xlarge instance type&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Product&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;CPU&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RAM&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Hourly Price&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Price per Month&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Amazon EC2 m4.2xlarge&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8 vCore&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32 GB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚Ç¨ 0,5059&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚Ç¨ 364,24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Scaleway Baremetal C2L&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8 Core&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32 GB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚Ç¨ 0,0333&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚Ç¨ 23,99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you can see the EC2 instance costs roughly 15 times what the Scaleway server costs.
Even if those two offerings are not 100% comparable I find the pricing of the Scaleway server very impressive.&lt;/p&gt;

&lt;p&gt;After introducing the new Scaleway C2 server let&amp;rsquo;s get started by setting up a couple of the C2 servers.&lt;/p&gt;

&lt;p&gt;If you do not have an account with Scaleway you need to &lt;a href=&#34;https://cloud.scaleway.com/#/signup&#34;&gt;sign up&lt;/a&gt; with them first.
Afterwards you can use the &lt;a href=&#34;https://github.com/scaleway/scaleway-cli&#34;&gt;Scaleway Command Line Interface&lt;/a&gt; to create the servers.&lt;/p&gt;

&lt;p&gt;Download the latest version for your operating system from the &lt;a href=&#34;https://github.com/scaleway/scaleway-cli/releases/tag/v1.8.0&#34;&gt;release page&lt;/a&gt;.
For working with the new C2 server you need at the latest version 1.8.0 of the Scaleway CLI.&lt;/p&gt;

&lt;p&gt;For Mac user the easiest way to install it is via &lt;a href=&#34;http://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install scw
==&amp;gt; Downloading https://homebrew.bintray.com/bottles/scw-1.7.1.mavericks.bottle.tar.gz
Already downloaded: /Library/Caches/Homebrew/scw-1.7.1.mavericks.bottle.tar.gz
==&amp;gt; Pouring scw-1.7.1.mavericks.bottle.tar.gz
==&amp;gt; Caveats
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d

zsh completion has been installed to:
  /usr/local/share/zsh/site-functions
==&amp;gt; Summary
üç∫  /usr/local/Cellar/scw/1.7.1: 4 files, 10.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if the Scaleway CLI works with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw --version
scw version v1.7.1, build homebrew
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good.&lt;/p&gt;

&lt;p&gt;To be able to do anything meaningful with the CLI we need first to log into our account.
Use the email and password you got when you registered as credentials.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw login
Login (cloud.scaleway.com): somemail@somewhere.com
Password:
Do you want to upload an SSH key ?
[0] I don&#39;t want to upload a key !
[3] id_rsa.pub
Which [id]:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you are asked to upload the public part of your SSH key you should do so.
This allows us later to securely connect to our Scaleway ARM servers via SSH.&lt;/p&gt;

&lt;p&gt;After a successful login we are now able to interact with our Scaleway account remotely.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s create three servers for our Swarm Cluster.
The C2 server type is currently in preview which means there is a quota that allows only to spin up a limited number of each instance type.
So I settled for two C2L and one C2M.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw create --commercial-type=C2L --name=cl-leader Docker
16ca1b31-f94e-4d6c-be46-ade4641054e4

$ scw create --commercial-type=C2L --name=&amp;quot;cl-follower1&amp;quot; Docker
16ca1b31-f94e-4d6c-be46-ade4641052e4

$ scw create --commercial-type=C2M --name=&amp;quot;cl-follower2&amp;quot; Docker
16ca1b31-f94e-4d6c-be46-ade4641053e4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That was easy, right?&lt;/p&gt;

&lt;p&gt;We can list our current servers by using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw ps
SERVER ID           IMAGE               COMMAND             CREATED             STATUS              PORTS               NAME
f20a3a57            Docker_1_10_0                           1 days              running             212.47.235.237      cl-leader
8cae4fd4            Docker_1_10_0                           1 days              running             163.172.135.05      cl-follower1
6al5b4e4            Docker_1_10_0                           1 days              running             163.172.135.28      cl-follower2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s check what we have by logging into one of our new servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader

root@cl-leader:~# docker info
root@cl-leader:~# docker info
Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 1.10.3
Storage Driver: aufs
 Root Dir: /var/lib/docker/aufs
 Backing Filesystem: extfs
 Dirs: 0
 Dirperm1 Supported: true
Execution Driver: native-0.2
Logging Driver: json-file
Plugins:
 Volume: local
 Network: bridge null host
Kernel Version: 4.4.5-docker-1
Operating System: Ubuntu 15.10
OSType: linux
Architecture: x86_64
CPUs: 8
Total Memory: 31.34 GiB
Name: cl-leader
ID: GUOI:LSEY:LASQ:XACT:M7D2:DVA3:LYLV:5YGT:6M7O:2P7T:5VXX:HKMO
Labels:
 provider=scaleway
Codename:wily
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we do have up-to-date software:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu 15.10&lt;/li&gt;
&lt;li&gt;a  4.4 Linux Kernel&lt;/li&gt;
&lt;li&gt;Docker 1.10.3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seems the Scaleway servers are a perfect place to run Docker and we have everything ready to move forward.&lt;/p&gt;

&lt;p&gt;The next step is setting up a private network between those three servers.
I am going to use a VPN solution called &lt;a href=&#34;https://www.tinc-vpn.org/&#34;&gt;tinc&lt;/a&gt; for that.
It is a really great peer-to-peer Mesh VPN solution, but basically you can use any VPN solution that works for you.
I am not going into the details of the tinc configuration now, because I am going to write about it in another blog post soon.&lt;/p&gt;

&lt;p&gt;So after configuring tinc we do have an additional network device on each server that is part of our private tinc network:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scw exec cl-leader &amp;quot; ip a | grep tun&amp;quot;
92: tun0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 500
    inet 10.0.0.2/24 scope global tun0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure that you can ping all the other nodes in your private network like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-follower1 &amp;quot;ping 10.0.0.2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That we have the name of tinc network device is important for the configuration of the Hypriot Cluster Lab which we gonna use in the next section.&lt;/p&gt;

&lt;h2 id=&#34;easy-swarming-with-the-hypriot-cluster-lab:42ed33cefda8a6f74e4e47b0527e1f17&#34;&gt;Easy swarming with the Hypriot Cluster Lab&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/hypriot/cluster-lab&#34;&gt;Hypriot Cluster Lab&lt;/a&gt; is the second reason why it is so easy to set up our 100 node Jenkins cluster.
The Cluster Lab helps us to set up a Docker Swarm Cluster without any manual configuration in minutes.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s configure the Hypriot package repository and install the Hypriot Cluster Lab.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;curl -s https://packagecloud.io/install/repositories/Hypriot/Schatzkiste/script.deb.sh | sudo bash; apt-get install -y hypriot-cluster-lab&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the Cluster Lab itself can be used in different scenarios we still need to configure some settings of the Cluster Lab to make it play nice with a VPN solution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server \&amp;quot;sed -i -e &#39;s|ENABLE_VLAN=&amp;quot;true&amp;quot;|ENABLE_VLAN=&amp;quot;false&amp;quot;|g&#39; -e &#39;s|ENABLE_DHCP=&amp;quot;true&amp;quot;|ENABLE_DHCP=&amp;quot;false&amp;quot;|g&#39; -e &#39;s|INTERFACE=&amp;quot;eth0&amp;quot;|INTERFACE=&amp;quot;tun0&amp;quot;|g&#39; /etc/cluster-lab/cluster.conf\&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are changing the Cluster Lab configuration to disable VLAN and DHCP support and we are also changing the Cluster Lab network interface to &amp;lsquo;tun0&amp;rsquo; which we created with tinc before.
Those settings ensure that the Cluster Lab uses our private tinc network for all cluster communication. That means also that all the Docker networks are created on top of the encrypted tinc network.&lt;/p&gt;

&lt;p&gt;The last thing we need to adjust is the Avahi daemon configuration. We need to adjust it for the peer-to-peer nature of the tinc network and restart the Avahi daemon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;sed -i &#39;s|#allow-point-to-point=no|allow-point-to-point=yes|g&#39; /etc/avahi/avahi-daemon.conf; systemctl restart avahi-daemon&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK. Now we are ready to start the Cluster Lab on the leader node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader &amp;quot;VERBOSE=true cluster-lab start&amp;quot;

Internet Connection
  [PASS]   tun0 exists
  [PASS]   tun0 has an ip address
  [PASS]   Internet is reachable
  [PASS]   DNS works

Networking
  [PASS]   tun0 exists
  [PASS]   Cluster leader is reachable
  [PASS]   tun0 has exactly one IP
  [PASS]   tun0 has no local link address
  [PASS]   Avahi process exists
  [PASS]   Avahi is using tun0

Docker
  [PASS]   Docker is running
  [PASS]   Docker is configured to use Consul as key-value store
  [PASS]   Docker is configured to listen via tcp at port 2375
  [PASS]   Docker listens on 10.0.0.2 via tcp at port 2375 (Docker-Engine)

Consul
  [PASS]   Consul Docker image exists
  [PASS]   Consul Docker container is running
  [PASS]   Consul is listening on port 8300
  [PASS]   Consul is listening on port 8301
  [PASS]   Consul is listening on port 8302
  [PASS]   Consul is listening on port 8400
  [PASS]   Consul is listening on port 8500
  [PASS]   Consul is listening on port 8600
  [PASS]   Consul API works
  [PASS]   Cluster-Node is pingable with IP 10.0.0.3
  [PASS]   Cluster-Node is pingable with IP 10.0.0.4
  [PASS]   Cluster-Node is pingable with IP 10.0.0.2
  [PASS]   No Cluster-Node is in status &#39;failed&#39;
  [PASS]   Consul is able to talk to Docker-Engine on port 7946 (Serf)

Swarm
  [PASS]   Swarm-Join Docker container is running
  [PASS]   Swarm-Manage Docker container is running
  [PASS]   Number of Swarm and Consul nodes is equal which means our cluster is healthy scw exec cl-leader &amp;quot;VERBOSE=true cluster-lab start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the &lt;code&gt;VERBOSE=true&lt;/code&gt; environment variable we tell the Cluster Lab to print out all the self-tests when it starts up.
If you see any failed tests it is usually a good idea to run a &lt;code&gt;cluster-lab health&lt;/code&gt; command, which does just the self-tests again.
Usually those should be green now and have only failed the first time because of timing issues.&lt;/p&gt;

&lt;p&gt;If you still have problems check our &lt;a href=&#34;https://github.com/hypriot/cluster-lab#troubleshooting&#34;&gt;trouble shooting&lt;/a&gt; section of the Cluster Lab project.&lt;/p&gt;

&lt;p&gt;It is important that the leader is started up first because the other nodes need him for the self-configuration.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s start the Cluster Lab on the rest of the nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;VERBOSE=true cluster-lab start&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now check if our Swarm Cluster is fully functional by running a &lt;code&gt;docker info&lt;/code&gt; against the Docker Swarm port:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader
root@cl-leader:~# DOCKER_HOST=tcp://10.0.0.2:2378 docker info
Containers: 20
 Running: 9
 Paused: 0
 Stopped: 11
Images: 27
Role: replica
Primary: 10.0.0.3:2378
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 cl-follower1: 10.0.0.3:2375
  ‚îî Status: Healthy
  ‚îî Containers: 7
  ‚îî Reserved CPUs: 0 / 8
  ‚îî Reserved Memory: 0 B / 32.91 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-20T15:07:55Z
 cl-follower2: 10.0.0.4:2375
  ‚îî Status: Healthy
  ‚îî Containers: 7
  ‚îî Reserved CPUs: 0 / 8
  ‚îî Reserved Memory: 0 B / 16.4 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=leader, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-20T15:08:09Z
 cl-leader: 10.0.0.2:2375
  ‚îî Status: Healthy
  ‚îî Containers: 6
  ‚îî Reserved CPUs: 0 / 8
  ‚îî Reserved Memory: 0 B / 32.91 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-20T15:07:52Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.4.5-docker-1
Operating System: linux
Architecture: amd64
CPUs: 24
Total Memory: 82.22 GiB
Name: 4b3fd589316c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the &lt;code&gt;docker info&lt;/code&gt; command now runs against the Swarm TCP port we get plenty of information about the Swarm cluster.
For instance we can see the number of nodes we have as well as the total number of CPUs and RAM we do have in the cluster: 24 CPUs and a whopping amount of 82 GB of RAM.
That&amp;rsquo;s nice isn&amp;rsquo;t it?&lt;/p&gt;

&lt;h2 id=&#34;starting-a-100-nodes-jenkins-cluster-with-docker-compose:42ed33cefda8a6f74e4e47b0527e1f17&#34;&gt;Starting a 100 Nodes Jenkins cluster with Docker-Compose&lt;/h2&gt;

&lt;p&gt;Ok, now comes the last reason why it is so fast and easy to set up 100 node Jenkins cluster.
The reason is called &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker-Compose&lt;/a&gt; and makes it ridiculous easy to get multi-container applications up and running.&lt;/p&gt;

&lt;p&gt;To use it we need to first install it on our cluster leader node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scw exec cl-leader &amp;quot;curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose; chmod +x /usr/local/bin/docker-compose&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a file called &amp;ldquo;docker-compose.yml&amp;rdquo; on the cluster leader with the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;version: &amp;quot;2&amp;quot;

networks:
  jenkins_swarm:
    driver: overlay
    ipam:
      driver: default
      config:
        - subnet: 10.10.2.0/16
          ip_range: 10.10.2.0/24

services:
  jenkins:
    image: csanchez/jenkins-swarm
    expose:
      - 8080
      - 50000
    ports:
      - 8080:8080
    restart: always
    networks:
      - jenkins_swarm
    environment:
      - &amp;quot;constraint:NODE==cl-leader&amp;quot;

  worker:
    image: csanchez/jenkins-swarm-slave
    command: -username jenkins -password jenkins -executors 1
    networks:
      - jenkins_swarm
    environment:
      - &amp;quot;JENKINS_PORT_8080_TCP_ADDR=jenkins&amp;quot;
      - &amp;quot;JENKINS_PORT_8080_TCP_PORT=8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Replace the &amp;lsquo;subnet&amp;rsquo; and &amp;lsquo;ip_range&amp;rsquo; attributes in the file with configuration values that fit your private network configuration.
This is a Docker-Compose file that describes an overlay network and two services: a Jenkins Master and a Jenkins worker.
The overlay network will stretch across our Swarm Cluster and runs on top of our private network.
Because of IP adress conflicts between our private tinc network and the standard network configuration we had to add the &amp;lsquo;ipam&amp;rsquo; part.&lt;/p&gt;

&lt;p&gt;Now we are nearly finished and can start some Jenkins nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@cl-leader:~# DOCKER_HOST=tcp://10.0.0.3:2378 docker-compose -p jenkins up -d
Starting jenkins_jenkins_1
Starting jenkins_worker_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here the important part is that the docker-compose command is running against our Swarm Cluster.
&amp;lsquo;-p&amp;rsquo; option creates a project namespace and the &amp;lsquo;-d&amp;rsquo; option tell Docker-Compose to start the container in the background.
This command initally created two container: one Jenkins master node and one worker.&lt;/p&gt;

&lt;p&gt;We now need to scale the worker nodes to 99 to finally reach our goal of a 100 nodes Jenkins Cluster.&lt;/p&gt;

&lt;p&gt;And that is really easy with Docker-Compose:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@cl-leader:~# COMPOSE_HTTP_TIMEOUT=180 DOCKER_HOST=tcp://10.0.0.3:2378 docker-compose -p jenkins scale worker=20
Creating and starting 2 ...
Creating and starting 3 ...
Creating and starting 4 ...
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If those twenty nodes are up repeat the command with 40, 60, 80 until you reach 99.
You could of course go for 99 at once but I experienced errors and timeouts so I prefer to take it slowly.&lt;/p&gt;

&lt;p&gt;To visit the GUI of the Jenkins master server we need to find out the IP address of our cluster leader node.
We can do that with the &lt;code&gt;inspect&lt;/code&gt; subcommand of the Scaleway CLI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw inspect leader | grep address
   &amp;quot;address&amp;quot;: &amp;quot;212.47.225.127&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the Jenkins master can be found at: &lt;a href=&#34;http://212.47.225.127:8080&#34;&gt;http://212.47.225.127:8080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should see a page similar to&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/100-nodes-jenkins/jenkins01.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And at &lt;a href=&#34;http://212.47.225.127:8080/computer&#34;&gt;http://212.47.225.127:8080/computer&lt;/a&gt; we can find all the slave nodes, too:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/100-nodes-jenkins/jenkins02.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So to wrap it up I can say that the combination of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the on-demand Scaleway Servers&lt;/li&gt;
&lt;li&gt;the Hypriot Cluster Lab&lt;/li&gt;
&lt;li&gt;Docker-Compose&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;allowed me to get the Jenkins Cluster running in a really short time.&lt;/p&gt;

&lt;p&gt;It might not be 10 minutes for you - especially if you first need to set up the Scaleway account and the CLI tool. But if everything is in place it can be done in 5 to 10 minutes.&lt;/p&gt;

&lt;p&gt;Hope you had some fun following along&amp;hellip;.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;Govinda &lt;a href=&#34;https://twitter.com/_beagile_&#34;&gt;@_beagile__&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exciting Docker Swarm experiments in the Hypriot Cluster Lab</title>
      <link>http://localhost:1313/post/exciting-docker-swarm-experiments-in-the-hypriot-cluster-lab/</link>
      <pubDate>Thu, 10 Mar 2016 23:00:00 +0100</pubDate>
      
      <guid>http://localhost:1313/post/exciting-docker-swarm-experiments-in-the-hypriot-cluster-lab/</guid>
      <description>&lt;p&gt;This post is - like the &lt;a href=&#34;http://blog.hypriot.com/post/how-to-setup-rpi-docker-swarm/&#34;&gt;last one&lt;/a&gt; - dedicated to the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-join-your-first-swarm/&#34;&gt;Docker #SwarmWeek&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you thought that it was pretty easy to set up a Swarm Cluster after our last post - you are going to be pretty surprised to find out that there is still room for improvement.&lt;/p&gt;

&lt;p&gt;With our &lt;a href=&#34;https://github.com/hypriot/cluster-lab&#34;&gt;Hypriot Cluster Lab&lt;/a&gt; it is possible to get up a Swarm Cluster with just one command.
Follow along to learn how this is possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/swarm-cluster-lab/cluster_lab.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;
&lt;div style=&#34;text-align:right; font-size: smaller&#34;&gt;Image courtesy of &lt;a href=&#34;https://www.flickr.com/photos/snre/10579433974/in/photolist-h7Sn93-4A4CPY-cr7gZ3-ySfs-Q2aKL-Q2GvM-akjGfr-6cX1D7-7Z4GYE-8XKoBo-82k2PZ-5J8yhX-7Sct35-h7SfKJ-h7SmSb-h7S44X-5J8y5a-4Y8HsH-p1Sz3h-4A4wom-4Bw54x-gQ23WX-iKWTQA-8D8m59-6PgZhg-8tKzAS-4kvY6K-9kaX8z-54x1BV-5xh7Sb-bLXPYZ-4zQ8mN-7KjCAt-MSdyd-qAw4Hw-awH1nc-4tKyxG-5wWtGU-4LMU7v-7NkBns-9a3pwX-h7Tuik-iAjqa-rvTgXa-h7S5uT-h7Shc1-qL7ofi-a18dLe-5RVXPz-8gLV&#34;&gt;SNRE Labs&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;The Hypriot Cluster Lab has already been there for while as an easy way to set up a Swarm Cluster on Raspberry Pi&amp;rsquo;s.
The basic idea was to have a SD card image that can be used to boot up a couple of Pi&amp;rsquo;s who then automatically form a Swarm Cluster.
As with any Lab the Cluster Lab provides a place to run all kind of experiments and gain new insights.&lt;/p&gt;

&lt;p&gt;As part of our recent refactoring efforts to make the Cluster Lab more reliable we have been searching for ways to test the Cluster Lab as a whole.
The idea we came up was to use &lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt; for our integration testing efforts.&lt;/p&gt;

&lt;p&gt;Vagrant is a tool that makes it really easy to manage Virtual Machines. It allows us to describe a multi-machine setup in an easy to read text-based configuration file:&lt;/p&gt;

&lt;script src=&#34;https://gist-it.appspot.com/github/hypriot/cluster-lab/raw/master/vagrant/Vagrantfile?slice=14:40&amp;footer=minimal&#34;&gt;&lt;/script&gt;

&lt;p&gt;One of the big advantages of Vagrant is that it is available for Mac, Linux and Windows users. This means that you can run the Cluster Lab on all those operating systems, too.&lt;/p&gt;

&lt;p&gt;To get started with the Hypriot Cluster Lab you need to ensure that you have a recent version of Vagrant installed.
If not you can download it &lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34;&gt;here&lt;/a&gt;. Under the hood Vagrant uses a so-called provider to run the virtual machines.
For the Cluster Lab the recommended way is to use &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox&lt;/a&gt;. Make sure it is already there or download and install it.&lt;/p&gt;

&lt;p&gt;The next step is to clone the Hypriot Cluster Lab repository like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/hypriot/cluster-lab.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Afterwards change into the &lt;code&gt;cluster-lab/vagrant&lt;/code&gt; folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd cluster-lab/vagrant
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now comes the single command you need to boot up a whole working Swarm Cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up --provider virtualbox --no-color
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will create three virtual machines based on Ubuntu Wily, install a recent Docker Engine and install and configure the Cluster Lab.
As this command will download Ubuntu Wily and lots of other software, too, it might take while. Please be patient.&lt;/p&gt;

&lt;p&gt;While the Cluster Lab is booting it will run a number of self-tests to ensure that it is working properly.
You should see output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Internet Connection
  [PASS]   eth1 exists
  [PASS]   eth1 has an ip address
  [PASS]   Internet is reachable
  [PASS]   DNS works

Networking
  [PASS]   eth1 exists
  [PASS]   vlan os package exists
  [PASS]   Avahi os package exists
  [PASS]   Avahi-utils os package exists
  [PASS]   Avahi process exists
  [PASS]   Avahi cluster-leader.service file is absent

Configure basic networking

Networking
  [PASS]   eth1.200 exists
  [PASS]   eth1.200 has correct IP from vlan network
  [PASS]   Cluster leader is reachable
  [PASS]   eth1.200 has exactly one IP
  [PASS]   eth1.200 has no local link address
  [PASS]   Avahi process exists
  [PASS]   Avahi is using eth1.200
  [PASS]   Avahi cluster-leader.service file exists

This node is Leader

DHCP is enabled

DNSmasq
  [PASS]   dnsmasq os package exists
  [PASS]   dnsmasq process exists
  [PASS]   /etc/dnsmasq.conf backup file is absent

Configure DNSmasq

DNSmasq
  [PASS]   dnsmasq process exists
  [PASS]   /etc/dnsmasq.conf backup file exists

Docker
  [PASS]   docker is installed
  [PASS]   Docker process exists
  [PASS]   /etc/default/docker backup file is absent

Configure Docker

Docker
  [PASS]   Docker is running
  [PASS]   Docker is configured to use Consul as key-value store
  [PASS]   Docker is configured to listen via tcp at port 2375
  [PASS]   Docker listens on 192.168.200.30 via tcp at port 2375 (Docker-Engine)

Consul
  [PASS]   Consul Docker image exists
  [PASS]   Consul Docker container is running
  [PASS]   Consul is listening on port 8300
  [PASS]   Consul is listening on port 8301
  [PASS]   Consul is listening on port 8302
  [PASS]   Consul is listening on port 8400
  [PASS]   Consul is listening on port 8500
  [PASS]   Consul is listening on port 8600
  [PASS]   Consul API works
  [PASS]   Cluster-Node is pingable with IP 192.168.200.30
  [PASS]   Cluster-Node is pingable with IP 192.168.200.45
  [PASS]   Cluster-Node is pingable with IP 192.168.200.1
  [PASS]   No Cluster-Node is in status &#39;failed&#39;
  [PASS]   Consul is able to talk to Docker-Engine on port 7946 (Serf)

Swarm
  [PASS]   Swarm-Join Docker container is running
  [PASS]   Swarm-Manage Docker container is running
  [PASS]   Number of Swarm and Consul nodes is equal which means our cluster is healthy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything worked all tests should be [PASS]ing.
In case that there were [FAIL]ing tests you can fix it with the help of our &lt;a href=&#34;https://github.com/hypriot/cluster-lab#troubleshooting&#34;&gt;troubleshooting&lt;/a&gt; guide.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;vagrant up&lt;/code&gt; command creates three cluster nodes called &lt;strong&gt;leader&lt;/strong&gt;, &lt;strong&gt;follower1&lt;/strong&gt; and &lt;strong&gt;follower2&lt;/strong&gt;.
To work with the Swarm Cluster we need to log into one of the cluster nodes and elevate ourself to root:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant ssh leader
$ sudo su
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use the Swarm Cluster by exporting the TCP address of the Swarm Cluster with the DOCKER_HOST environment variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://192.168.200.1:3278
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all our Docker commands will work against the Swarm Cluster instead of the local Docker Engine.&lt;/p&gt;

&lt;p&gt;Lets check if we really have a 3-Node-Swarm Cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
Containers: 9
 Running: 9
 Paused: 0
 Stopped: 0
Images: 6
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 follower1: 192.168.200.46:2375
  ‚îî Status: Healthy
  ‚îî Containers: 3
  ‚îî Reserved CPUs: 0 / 1
  ‚îî Reserved Memory: 0 B / 1.018 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-10T21:24:07Z
 follower2: 192.168.200.44:2375
  ‚îî Status: Healthy
  ‚îî Containers: 3
  ‚îî Reserved CPUs: 0 / 1
  ‚îî Reserved Memory: 0 B / 1.018 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-10T21:24:35Z
 leader: 192.168.200.1:2375
  ‚îî Status: Healthy
  ‚îî Containers: 3
  ‚îî Reserved CPUs: 0 / 1
  ‚îî Reserved Memory: 0 B / 1.018 GiB
  ‚îî Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=leader, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-10T21:24:04Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.2.0-30-generic
Operating System: linux
Architecture: amd64
CPUs: 3
Total Memory: 3.054 GiB
Name: 9c2606252982
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we really have a working Swarm Cluster running.
That was easy, was it?&lt;/p&gt;

&lt;p&gt;For more information about Docker Swarm you can follow the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-container-orchestration-docker-swarm/&#34;&gt;#SwarmWeek: Introduction to container orchestration with Docker Swarm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback, discuss this post on &lt;a href=&#34;https://news.ycombinator.com/item?id=11263205&#34;&gt;HackerNews&lt;/a&gt; and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;Govinda &lt;a href=&#34;https://twitter.com/_beagile_&#34;&gt;@_beagile__&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to setup a Docker Swarm cluster with Raspberry Pi&#39;s</title>
      <link>http://localhost:1313/post/how-to-setup-rpi-docker-swarm/</link>
      <pubDate>Tue, 08 Mar 2016 17:27:58 +0100</pubDate>
      
      <guid>http://localhost:1313/post/how-to-setup-rpi-docker-swarm/</guid>
      <description>

&lt;p&gt;This week is dedicated to &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-join-your-first-swarm/&#34;&gt;Docker #SwarmWeek&lt;/a&gt;. In this tutorial we show you how easy it is to setup a Docker Swarm with HypriotOS and the standard &lt;code&gt;docker-machine&lt;/code&gt; binary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/how-to-setup-rpi-docker-swarm/docker-swarm-logo.png&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We want to setup a cluster with &lt;strong&gt;eight Raspberry Pi 3&lt;/strong&gt;, grouped into two tiny datacenters with four Pi&amp;rsquo;s each.&lt;/p&gt;

&lt;h2 id=&#34;prepare-your-notebook:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Prepare your notebook&lt;/h2&gt;

&lt;p&gt;To control the Docker Swarm from our notebook, we have to install both &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-machine&lt;/code&gt; binaries. If you are on a Mac, you can use &lt;code&gt;brew&lt;/code&gt; to install them.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install docker
brew install docker-machine
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;flash-all-your-sd-cards:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Flash all your SD cards&lt;/h2&gt;

&lt;p&gt;Now we can easily flash the latest HypriotOS image to the eight SD cards with our &lt;a href=&#34;https://github.com/hypriot/flash#installation&#34;&gt;flash tool&lt;/a&gt; and assign different node names.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flash --hostname swarm-dc1-pi01 https://downloads.hypriot.com/hypriot-rpi-20160306-192317.img.zip
flash --hostname swarm-dc1-pi02 https://downloads.hypriot.com/hypriot-rpi-20160306-192317.img.zip
...
flash --hostname swarm-dc2-pi04 https://downloads.hypriot.com/hypriot-rpi-20160306-192317.img.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, insert the SD cards and boot all the Raspberry Pi&amp;rsquo;s.&lt;/p&gt;

&lt;h2 id=&#34;prepare-your-pi-s-for-docker-machine:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Prepare your Pi&amp;rsquo;s for docker-machine&lt;/h2&gt;

&lt;p&gt;We want to create the Docker Swarm with the standard &lt;code&gt;docker-machine&lt;/code&gt; binary. To make this work we need to prepare the Raspberry Pi&amp;rsquo;s a little bit. The next steps are adding your SSH public key to all of your Pi&amp;rsquo;s as well as fixing the ID in &lt;code&gt;/etc/os-release&lt;/code&gt; to &lt;code&gt;debian&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function getip() { (traceroute $1 2&amp;gt;&amp;amp;1 | head -n 1 | cut -d\( -f 2 | cut -d\) -f 1) }

IP_ADDRESS=$(getip swarm-dc1-pi01.local)

ssh-keygen -R $IP_ADDRESS
ssh-copy-id -oStrictHostKeyChecking=no -oCheckHostIP=no root@$IP_ADDRESS

ssh root@$IP_ADDRESS sed -i \&#39;s/ID=raspbian/ID=debian/g\&#39; /etc/os-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Repeat this for all the eight Raspberry Pi&amp;rsquo;s. You will prompted for the root password which is &lt;code&gt;hypriot&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-swarm-token:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Create Swarm Token&lt;/h2&gt;

&lt;p&gt;A Docker Swarm cluster uses a unique Cluster ID to enable all swarm agents find each other. We need such a Cluster ID to build our Docker Swarm. This can be done in your shell with this command&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export TOKEN=$(for i in $(seq 1 32); do echo -n $(echo &amp;quot;obase=16; $(($RANDOM % 16))&amp;quot; | bc); done; echo)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-the-swarm-with-docker-machine:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Create the swarm with docker-machine&lt;/h2&gt;

&lt;p&gt;We now create the Swarm Master on the first Raspberry Pi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-machine create -d generic \
  --engine-storage-driver=overlay --swarm --swarm-master \
  --swarm-image hypriot/rpi-swarm:latest \
  --swarm-discovery=&amp;quot;token://$TOKEN&amp;quot; \
  --generic-ip-address=$(getip swarm-dc1-pi01.local) \
  swarm-dc1-pi01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For all the seven remaining Raspberry Pi&amp;rsquo;s we create Swarm Agents that join the Swarm cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-machine create -d generic \
  --engine-storage-driver=overlay --swarm \
  --swarm-image hypriot/rpi-swarm:latest \
  --swarm-discovery=&amp;quot;token://$TOKEN&amp;quot; \
  --generic-ip-address=$(getip swarm-dc1-pi02.local) \
  swarm-dc1-pi02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a while the whole Docker Swarm cluster with the two datacenters is up and running. We now have a cluster with &lt;strong&gt;32 CPU&amp;rsquo;s&lt;/strong&gt; and &lt;strong&gt;8 GByte RAM&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;control-your-swarm:7086adb18de3c46a0c9331e9b3dc5f01&#34;&gt;Control your Swarm&lt;/h2&gt;

&lt;p&gt;To connect to your Docker Swarm use the following command to set the environment variables for the Docker Client.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eval $(docker-machine env --swarm swarm-dc1-pi01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/how-to-setup-rpi-docker-swarm/dc1-pi3.jpg&#34; alt=&#34;Datacenter 1&#34; /&gt;
&lt;img src=&#34;http://localhost:1313/images/how-to-setup-rpi-docker-swarm/dc2-pi3.jpg&#34; alt=&#34;Datacenter 2&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker info
Containers: 10
 Running: 9
 Paused: 0
 Stopped: 1
Images: 8
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 8
 swarm-dc1-pi01: 192.168.1.207:2376
  ‚îî Status: Healthy
  ‚îî Containers: 3
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:47:03Z
 swarm-dc1-pi02: 192.168.1.209:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:56Z
 swarm-dc1-pi03: 192.168.1.206:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:22Z
 swarm-dc1-pi04: 192.168.1.208:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:51Z
 swarm-dc2-pi01: 192.168.1.204:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:35Z
 swarm-dc2-pi02: 192.168.1.205:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:49Z
 swarm-dc2-pi03: 192.168.1.210:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:40Z
 swarm-dc2-pi04: 192.168.1.211:2376
  ‚îî Status: Healthy
  ‚îî Containers: 1
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.17-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), provider=generic, storagedriver=overlay
  ‚îî Error: (none)
  ‚îî UpdatedAt: 2016-03-08T17:46:37Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.1.17-hypriotos-v7+
Operating System: linux
Architecture: arm
CPUs: 32
Total Memory: 7.592 GiB
Name: d90d49c65205
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/how-to-setup-rpi-docker-swarm/five-pi3.jpg&#34; alt=&#34;five pi3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For more information about Docker Swarm you can follow the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-container-orchestration-docker-swarm/&#34;&gt;#SwarmWeek: Introduction to container orchestration with Docker Swarm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always use the comments below to give us feedback and share it on Twitter or Facebook.&lt;/p&gt;

&lt;p&gt;Dieter &lt;a href=&#34;https://twitter.com/quintus23m&#34;&gt;@Quintus23M&lt;/a&gt; &amp;amp; Stefan &lt;a href=&#34;https://twitter.com/stefscherer&#34;&gt;@stefscherer&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Hypriot Cluster Lab: Docker clustering as easy as it gets</title>
      <link>http://localhost:1313/post/introducing-hypriot-cluster-lab-docker-clustering-as-easy-as-it-gets/</link>
      <pubDate>Tue, 08 Dec 2015 15:30:00 +0200</pubDate>
      
      <guid>http://localhost:1313/post/introducing-hypriot-cluster-lab-docker-clustering-as-easy-as-it-gets/</guid>
      <description>

&lt;p&gt;Today we wanna share something with you that we have been working on for the last couple of weeks. And we are pretty exited about it, too.
It is based on our beloved &lt;a href=&#34;http://blog.hypriot.com/post/get-your-all-in-one-docker-playground-now-hypriotos-reloaded/&#34;&gt;HypriotOS&lt;/a&gt; and makes it dead simple to build Docker clusters.&lt;/p&gt;

&lt;p&gt;Until now it was not exactly easy to get started with Docker clustering.
You would have needed specific knowledge and lots of time to manually configure the cluster and its individual nodes.&lt;/p&gt;

&lt;p&gt;Well, that&amp;rsquo;s now a thing of the past.&lt;/p&gt;

&lt;p&gt;May we introduce to you the newest member of the Hypriot family: &lt;strong&gt;The Hypriot Cluster Lab!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cluster-lab-release-v01/cluster_lab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the &lt;strong&gt;Hypriot Cluster Lab&lt;/strong&gt; it is just a matter of minutes to set up your own personal Docker cluster.
All you need is a couple of Raspberry Pi&amp;rsquo;s - 3, 5, 30 or even 100 - it is up to you - and our Hypriot Cluster Lab SD card image.&lt;/p&gt;

&lt;p&gt;We designed the Cluster Lab to be completely self-configuring, so there is nothing to configure or to set up.
Basically you just need to download our Cluster Lab SD card image and flash it onto a number of SD cards.
Then ensure that all your Pi&amp;rsquo;s have network connectivity, insert the SD cards and switch on power.
Everything else is taken care of automatically by our Cluster Lab.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker Clustering as easy as it gets!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When we started out to develop the Cluster Lab we wanted to be able to create complex Raspberry Pi based clusters with an arbitrary number of nodes.
We wanted to directly jump to deploying all kind of interesting services on top of the cluster instead of being concerned with setting up the cluster itself.&lt;/p&gt;

&lt;p&gt;And well - we managed to pull this off by combining a number of great technologies.
For instance &lt;strong&gt;Avahi&lt;/strong&gt; for announcing/managing who is a master and who is a slave node in the cluster. &lt;strong&gt;VLAN&lt;/strong&gt; for isolating the cluster network from other existing networks.
&lt;strong&gt;DHCP&lt;/strong&gt; for automatically assigning IP addresses to slave nodes in the cluster network. &lt;strong&gt;Consul&lt;/strong&gt; as a service registry and key-value-store.
And of course a number of other Docker related technologies that we already provide in HypriotOS: &lt;strong&gt;Docker Engine&lt;/strong&gt;, &lt;strong&gt;Swarm&lt;/strong&gt; and &lt;strong&gt;Compose&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;These technologies work together seamlessly and form what we call the Hypriot Cluster Lab. On top of it we are now able to easily deploy all kind of cluster services.
We have a number of ideas where this can come in handy in the future! Think Kubernetes for instance or Redis cluster.&lt;/p&gt;

&lt;p&gt;The Cluster Lab is still a bit rough around the edges and is more technology preview than production ready software, but we think it demonstrates the basic use case very well and shows the potential.
So for the coming weeks we want to gather feedback and make it more polished and resilient.
After that our main goal is to make it possible that all kind of cluster scenarios can be deployed on top of the Cluster Lab with just one command.
We want to make this possible by providing a kind of plugin-mechanism so that the community can help us in enabling many more interesting cluster use cases.&lt;/p&gt;

&lt;p&gt;The main reason that makes us really excited about the Cluster Lab, is that we think that there is great potential in using it as an educational tool in schools, universities or in commercial trainings.
It can be used to teach about Linux, Networking, Clustering, Microservices and so much more!&lt;/p&gt;

&lt;p&gt;And with the latest member of the Raspberry Pi family - the &lt;a href=&#34;http://swag.raspberrypi.org/collections/pi-zero/products/pi-zero&#34;&gt;Pi Zero&lt;/a&gt; - it got really cheap to have your own cluster. For about 50 bucks you are able to have a two to three node physical cluster.
And believe us - having physical nodes and being able to pull the network or power to simulate different cluster scenarios makes all the difference.
Working with &lt;strong&gt;real hardware&lt;/strong&gt; compared to a virtual environment (e.g. Vagrant) &lt;strong&gt;has a certain raw and primal feel about it&lt;/strong&gt; that we really like. :)&lt;/p&gt;

&lt;p&gt;So enough talking - let&amp;rsquo;s get our hands dirty - shall we?&lt;/p&gt;

&lt;h3 id=&#34;prerequisites-or-what-you-gonna-need-to-follow-along:8280d5ff2f6dfae78180ad5a72400c1a&#34;&gt;Prerequisites or what you gonna need to follow along&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;at least two &lt;strong&gt;Raspberry Pi&amp;rsquo;s&lt;/strong&gt;: Model 1 or 2 - both will do&lt;/li&gt;
&lt;li&gt;for each Raspberry Pi a &lt;strong&gt;power supply&lt;/strong&gt;, a &lt;strong&gt;MicroSD card&lt;/strong&gt; and a &lt;strong&gt;network cable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;network switch&lt;/strong&gt; that is somehow connected to the Internet and a DHCP server; both is usually provided by your typical off-the-shelf home router&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, the switch should not filter IEEE 802.1Q VLAN flags out of network packets. Usually this feature is provided even by cheap switches. If you wanna be safe, go through a small test to figure this out. The test is &lt;a href=&#34;https://github.com/hypriot/cluster-lab/blob/master/README.md#troubleshooting&#34;&gt;listed here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;download-flash-boot-enjoy:8280d5ff2f6dfae78180ad5a72400c1a&#34;&gt;Download. Flash. Boot. Enjoy!&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; &lt;strong&gt;Download&lt;/strong&gt; the &lt;a href=&#34;http://downloads.hypriot.com/hypriot_20160121-235123_clusterlab.img.zip&#34;&gt;Hypriot Cluster Lab SD card image&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Flash the image to your SD cards your way or use &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;our funky flash script&lt;/a&gt; which makes flashing the SD cards so much easier.&lt;/p&gt;

&lt;p&gt;Another advantage of our flash script is that it also allows you to give your cluster nodes unique hostnames:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ flash --hostname cl-master http://downloads.hypriot.com/hypriot_20160121-235123_clusterlab.img.zip
$ flash --hostname cl-node-1 http://downloads.hypriot.com/hypriot_20160121-235123_clusterlab.img.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Put the freshly flashed SD cards in each node&amp;rsquo;s SD card slot.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Power on &lt;strong&gt;only one&lt;/strong&gt; node. This node will automatically become the master node of the cluster. It might take up to two minutes until the master node is fully functional.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt; Find out the IP address of your master node. One way to do this is via &lt;a href=&#34;https://nmap.org/&#34;&gt;nmap&lt;/a&gt; and is described &lt;a href=&#34;http://blog.hypriot.com/getting-started-with-docker-and-linux-on-the-raspberry-pi/#ensure-everything-works:8814904f208dcaade82991443c7514e0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt; Use the IP (from step 5) or the hostname (from step 2) to point your browser to &lt;code&gt;http://{IP or hostname of the master node}:8500&lt;/code&gt;. In our case &lt;code&gt;http://cl-master:8500&lt;/code&gt; opens the Consul web interface and our cluster master node is displayed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cluster-lab-release-v01/consul_cl_master.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Power on all the remaining cluster nodes only if step 5 was successful. After about 2 minutes you should see the rest of them being listed in the Consul web interface, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cluster-lab-release-v01/consul_cl_master_and_nodes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The last step makes the cluster fully operational and we are now able to work with the cluster in earnest.&lt;/p&gt;

&lt;h3 id=&#34;babysteps-with-our-cluster-lab:8280d5ff2f6dfae78180ad5a72400c1a&#34;&gt;Babysteps with our Cluster Lab&lt;/h3&gt;

&lt;p&gt;Congratulations, you got your Hypriot Cluster Lab up and running! That was easy, wasn&amp;rsquo;t it?&lt;/p&gt;

&lt;p&gt;The Cluster Lab is using &lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Docker Swarm&lt;/a&gt; for managing Docker containers on the nodes that make up the cluster.
Docker Swarm will distribute containers based on different distribution &lt;a href=&#34;https://docs.docker.com/swarm/scheduler/strategy/&#34;&gt;strategies&lt;/a&gt; to individual nodes.
Per default Docker Swarm uses the &lt;em&gt;spread&lt;/em&gt; strategy to evenly distribute container on cluster nodes.&lt;/p&gt;

&lt;p&gt;Working with Docker Swarm is easy. To start we first need to log into our cluster master:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh root@cl-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There we can use the Docker Client to connect to the Swarm Manager instance. We do that by providing the &amp;lsquo;-H&amp;rsquo; flag to the &lt;code&gt;docker&lt;/code&gt; command.
This enables the Docker client to use the Docker Remote API for accessing the Swarm Manager.&lt;/p&gt;

&lt;p&gt;To display some basic info about the Swarm Cluster run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 info
Containers: 7
Images: 6
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 cl-master: 192.168.200.1:2375
  ‚îî Containers: 3
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.12-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), storagedriver=overlay
 cl-node-1: 192.168.200.115:2375
  ‚îî Containers: 2
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.12-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), storagedriver=overlay
 cl-node-2: 192.168.200.113:2375
  ‚îî Containers: 2
  ‚îî Reserved CPUs: 0 / 4
  ‚îî Reserved Memory: 0 B / 971.8 MiB
  ‚îî Labels: executiondriver=native-0.2, kernelversion=4.1.12-hypriotos-v7+, operatingsystem=Raspbian GNU/Linux 8 (jessie), storagedriver=overlay
CPUs: 12
Total Memory: 2.84
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of the hostname &lt;code&gt;cl-master&lt;/code&gt; one can also use the IP address that is always fixed for the cluster master node: 192.168.200.1.&lt;/p&gt;

&lt;p&gt;Ok - it seems our Swarm cluster is truly up and running.&lt;/p&gt;

&lt;p&gt;Time to get a little bit more daring.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s spin up a web interface for managing our nodes called &lt;a href=&#34;https://github.com/crosbymichael/dockerui&#34;&gt;DockerUI&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H cl-master:2378 run -d -p 9000:9000 --env=&amp;quot;constraint:node==cl-master&amp;quot; --name dockerui hypriot/rpi-dockerui -e http://192.168.200.1:2378
51f2eb09ab48540eb4a052bbe07644487c3a0b29ca44a6217ea6aebf17b3df0c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The most interesting part here is the env parameter &lt;code&gt;--env=&amp;quot;constraint:node==cl-master&amp;quot;&lt;/code&gt; which tells the Swarm Manager that we want to start our new container on the &lt;strong&gt;cl-master&lt;/strong&gt; node.
Without that the new container would be started by Docker Swarm on one of the nodes according to the &lt;em&gt;spread&lt;/em&gt; strategy.
By using the &amp;lsquo;constraint:node&amp;rsquo; label we are able to control on which node a container gets started.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s open the DockerUI with the following URL: &lt;code&gt;http://cl-master:9000&lt;/code&gt;.
If everything did work you should now see an overview of your running containers similar to this one:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/cluster-lab-release-v01/dockerui.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Back to the command line we can see the same result by using the &lt;code&gt;docker ps&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 ps
CONTAINER ID        IMAGE                                                              COMMAND                  CREATED             STATUS              PORTS                          NAMES
51f2eb09ab48        hypriot/rpi-dockerui                                               &amp;quot;./dockerui -e http:/&amp;quot;   12 minutes ago      Up 12 minutes       192.168.200.1:9000-&amp;gt;9000/tcp   cl-master/dockerui
fca75c6b759a        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   About an hour ago   Up About an hour                                   cl-node-2/bin_consul_1
4bfa58ed2a07        hypriot/rpi-swarm                                                  &amp;quot;/swarm join --advert&amp;quot;   About an hour ago   Up About an hour    2375/tcp                       cl-node-2/bin_swarm_1
ec61f8f5d766        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   About an hour ago   Up About an hour                                   cl-node-1/bin_consul_1
75c7cb003639        0104b3a10aad7e9a3d38ca4dce652c73d195b87171675c7dbc114ae85a444831   &amp;quot;/swarm join --advert&amp;quot;   About an hour ago   Up About an hour    2375/tcp                       cl-node-1/bin_swarm_1
df027cd23e69        hypriot/rpi-swarm                                                  &amp;quot;/swarm manage consul&amp;quot;   2 hours ago         Up 2 hours          192.168.200.1:2378-&amp;gt;2375/tcp   cl-master/bin_swarmmanage_1
f6b11e9e4f07        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   2 hours ago         Up 2 hours                                         cl-master/bin_consul_1
8658010a4433        hypriot/rpi-swarm                                                  &amp;quot;/swarm join --advert&amp;quot;   2 hours ago         Up 2 hours          2375/tcp                       cl-master/bin_swarm_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By now you should have gotten the hang of it and come to expect that you can use many of the Docker command line commands with Swarm, too.
And you are right - you just need to remember to use the &lt;code&gt;-H&lt;/code&gt; flag as part of the Docker command.&lt;/p&gt;

&lt;h3 id=&#34;getting-to-the-grown-up-stuff:8280d5ff2f6dfae78180ad5a72400c1a&#34;&gt;Getting to the grown-up stuff&lt;/h3&gt;

&lt;p&gt;After we did our first babysteps successfully it is now time for some serious grown-up stuff.
Certainly Docker multi-host networking can be considered serious stuff - don&amp;rsquo;t you think?&lt;/p&gt;

&lt;p&gt;First let&amp;rsquo;s see if we already have any networks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 network ls
NETWORK ID          NAME                DRIVER
d88253054dd4        cl-node-1/none      null
e78f9fc77a31        cl-node-2/bridge    bridge
12d2cb0e387d        cl-node-2/none      null
020bdb74ea43        cl-node-1/host      host
b39702828ebf        cl-node-1/bridge    bridge
c24764cf7077        cl-master/host      host
480319fbca22        cl-node-2/host      host
e5d7f7a69313        cl-master/bridge    bridge
7153745ef548        cl-master/none      null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are the networks that are already present by default.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s add our own overlay network:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 network create -d overlay my-net
54583b74b0c5b678678db18b99a1148049640e3c4e6ac6f5cdfa0938b1399f3a
HypriotOS: root@cl-master in ~
$ docker -H cl-master:2378 network ls
NETWORK ID          NAME                DRIVER
7153745ef548        cl-master/none      null
c24764cf7077        cl-master/host      host
54583b74b0c5        my-net              overlay
480319fbca22        cl-node-2/host      host
e5d7f7a69313        cl-master/bridge    bridge
b39702828ebf        cl-node-1/bridge    bridge
d88253054dd4        cl-node-1/none      null
e78f9fc77a31        cl-node-2/bridge    bridge
12d2cb0e387d        cl-node-2/none      null
020bdb74ea43        cl-node-1/host      host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we now have successfully created our first Docker multi-node overlay network.
This overlay network is really useful. Any container started in this network can talk to any other container in the network by default.&lt;/p&gt;

&lt;p&gt;In order to see how this works we are going to start two containers on different cluster nodes that will talk to each other.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 run -itd --name=webserver --net=my-net --env=&amp;quot;constraint:node==cl-node-1&amp;quot; hypriot/rpi-nano-httpd
378ddbe05781360f57f869f9aec7ad4c2cd703047cb5da11a9a7f395501bc533
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Listing the running containers in our cluster we now have:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 ps
CONTAINER ID        IMAGE                                                              COMMAND                  CREATED             STATUS              PORTS                          NAMES
378ddbe05781        hypriot/rpi-nano-httpd                                             &amp;quot;/httpd 80&amp;quot;              26 seconds ago      Up 23 seconds       80/tcp                         cl-node-1/webserver
51f2eb09ab48        hypriot/rpi-dockerui                                               &amp;quot;./dockerui -e http:/&amp;quot;   41 minutes ago      Up 40 minutes       192.168.200.1:9000-&amp;gt;9000/tcp   cl-master/dockerui
fca75c6b759a        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   About an hour ago   Up About an hour                                   cl-node-2/bin_consul_1
4bfa58ed2a07        hypriot/rpi-swarm                                                  &amp;quot;/swarm join --advert&amp;quot;   About an hour ago   Up About an hour    2375/tcp                       cl-node-2/bin_swarm_1
ec61f8f5d766        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   About an hour ago   Up About an hour                                   cl-node-1/bin_consul_1
75c7cb003639        0104b3a10aad7e9a3d38ca4dce652c73d195b87171675c7dbc114ae85a444831   &amp;quot;/swarm join --advert&amp;quot;   2 hours ago         Up 2 hours          2375/tcp                       cl-node-1/bin_swarm_1
df027cd23e69        hypriot/rpi-swarm                                                  &amp;quot;/swarm manage consul&amp;quot;   3 hours ago         Up 3 hours          192.168.200.1:2378-&amp;gt;2375/tcp   cl-master/bin_swarmmanage_1
f6b11e9e4f07        hypriot/rpi-consul                                                 &amp;quot;/consul agent -serve&amp;quot;   3 hours ago         Up 3 hours                                         cl-master/bin_consul_1
8658010a4433        hypriot/rpi-swarm                                                  &amp;quot;/swarm join --advert&amp;quot;   3 hours ago         Up 3 hours          2375/tcp                       cl-master/bin_swarm_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Everything so far looks good. So let&amp;rsquo;s get the final piece working by starting a web client that talks to our webserver.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H cl-master:2378 run -it --rm --net=my-net --env=&amp;quot;constraint:node==cl-node-2&amp;quot; hypriot/armhf-busybox wget -O- http://webserver/index.html
Connecting to webserver (10.0.0.2:80)
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Pi armed with Docker by Hypriot&amp;lt;/title&amp;gt;
  &amp;lt;body style=&amp;quot;width: 100%; background-color: black;&amp;quot;&amp;gt;
    &amp;lt;div id=&amp;quot;main&amp;quot; style=&amp;quot;margin: 100px auto 0 auto; width: 800px;&amp;quot;&amp;gt;
      &amp;lt;img src=&amp;quot;pi_armed_with_docker.jpg&amp;quot; alt=&amp;quot;pi armed with docker&amp;quot; style=&amp;quot;width: 800px&amp;quot;&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
-                    100% |*******************************|   304   0:00:00 ETA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we have been able to spin up a busybox container on another node.
We used it to fetch the index.html page with the &lt;code&gt;wget&lt;/code&gt; command from our webserver container.&lt;/p&gt;

&lt;p&gt;The simplicity of this illustrates how powerful Docker networking has become.
Creating this kind of a setup with - for example &lt;a href=&#34;http://openvswitch.org/&#34;&gt;OpenVSwitch&lt;/a&gt; - was way more complicated in the past.&lt;/p&gt;

&lt;p&gt;It is possible to create far more complex scenarios with our Cluster Lab, but hopefully we were able to demonstrate a bit of the potential it has.
We will write more about those in some future blog posts.&lt;/p&gt;

&lt;p&gt;Until then we hope that it was fun to follow along and that we could infect you a little bit with our passion for Docker clustering.&lt;/p&gt;

&lt;p&gt;You can find the source code of the Hypriot Cluster Lab at &lt;a href=&#34;https://github.com/hypriot/cluster-lab&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback, discuss this post on &lt;a href=&#34;https://news.ycombinator.com/item?id=10696752&#34;&gt;HackerNews&lt;/a&gt; and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;Andreas &amp;amp; Mathias&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>